{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e88da0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3a11fa",
   "metadata": {},
   "source": [
    "### Using B.E.R.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5643775c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install huggingface-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80484b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5eb083ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7593fc22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "# Load the summarization and question-answering pipelines\n",
    "summarizer = pipeline(\"summarization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19da2870",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pipeline = pipeline(\"question-answering\", model=\"distilbert-base-uncased-distilled-squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "520ef0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_answer(context, question):\n",
    "    # Summarize the context\n",
    "    summary = summarizer(context, max_length=130, min_length=30, do_sample=False)[0]['summary_text']\n",
    "    \n",
    "    # Get the answer to the question based on the summarized context\n",
    "    answer = qa_pipeline(question=question, context=summary)\n",
    "    return answer['answer']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45c9c55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"\n",
    "deep was coming to the class then he met karim on the way. But karim said not to go to the class since they were going to a movie with rohan.\n",
    "Rohan called vinay as he did not want to be a third wheel in their relationship. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e009a23f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 130, but your input_length is only 64. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vinay\n"
     ]
    }
   ],
   "source": [
    "\n",
    "question = \"who was in the relationship?\"\n",
    "\n",
    "# Get the answer\n",
    "answer = get_answer(context, question)\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd5468a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f984cebd",
   "metadata": {},
   "source": [
    "#### Using T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70e6e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "model_name = 't5-small'\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7b0a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def answer_question(context, question):\n",
    "    input_text = \"context: \" + context + \" \" + \"question: \" + question\n",
    "    input_ids = tokenizer(input_text, return_tensors='pt').input_ids\n",
    "    outputs = model.generate(input_ids)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b230748",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4c4059",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\n",
    "\n",
    "answer = answer_question(context, question)\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca92c42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
